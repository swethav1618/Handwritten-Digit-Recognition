# Handwritten-Digit-Recognition
Implementation of a machine learning model for handwritten digit recognition using the MNIST dataset and Jupyter Notebook.

---
## ğŸ“‚ Dataset Handling

- Dataset: [MNIST](http://yann.lecun.com/exdb/mnist/)  
- Preprocessing:
  - Images resized to **28x28 grayscale**.
  - Normalized pixel values.
   
## Custom Dataset Class

The custom dataset class acts like a bridge between raw data (MNIST images) and the training loop. It ensures the model gets data in the correct format, one batch at a time, with proper labels.
We implement a dataset class using `torch.utils.data.Dataset`, including:

- **`__init__()`** â€“ Loads the dataset.  
- **`__len__()`** â€“ Returns dataset length.  
- **`__getitem__()`** â€“ Retrieves an image-label pair at a given index
  
Together, these methods make the dataset compatible with **DataLoader**, which is responsible for batching and shuffling during training using torch.utils.data.DataLoader.

## ğŸ§  CNN Model Architecture, what does Convolution mean in CNNs?
In the context of CNNs, convolution is a mathematical operation where a small filter (also called a kernel) slides over the input image and performs element-wise multiplication and summation. This helps extract local patterns like edges, textures, or shapes.
Think of it like using a magnifying glass to scan different parts of an imageâ€”each filter is looking for a specific kind of feature.

- Our CNN uses multiple convolutional and pooling layers with ReLU activation.
- Convolutional layers â€“ extract features from input/digit images.
- ReLU â€“ introduces non-linearity.
- Max Pooling â€“ reduces spatial dimensions while preserving important features.
- Fully connected layers â€“ map extracted features to class probabilities.

## ğŸ§  Layer-by-Layer Transformation
- conv1 â†’ relu â†’ pool

  Conv1: (1, 28, 28) â†’ (32, 26, 26) (because kernel=3, padding=0, stride=1).

  ReLU: applies elementwise â†’ no shape change.

  MaxPool(2,2): halves H and W â†’ (32, 13, 13).
  
- conv2 â†’ relu â†’ pool
  
  Conv2: (32, 13, 13) â†’ (64, 11, 11) (kernel=3).
  
  ReLU â†’ same shape.

  MaxPool(2,2) â†’ (64, 5, 5).

- Flatten

  (64, 5, 5) â†’ flattened into 64 Ã— 5 Ã— 5 = 1600 features.
  
  So shape becomes (1, 1600).

- fc1 (Linear layer) 1600 â†’ 128.
  
  Shape: (1, 128).

- fc2 (final Linear layer)
  
  128 â†’ 10.

- Shape: (1, 10).

## âš™ï¸ Training
- Loss Function: CrossEntropyLoss is used to measure classification error.  
- Optimizer: Adam is chosen for efficient weight updates.
- Training is performed for several epochs using mini-batches.  
  
## ğŸ“Š Results
- Evaluation Metrics: Accuracy, Confusion Matrix
- Final Test Accuracy: ~99% ğŸ‰
- The model quickly converges and reaches high accuracy.


## ğŸ”® Future Work
- Experiment with deeper CNN architectures.
- Add dropout/regularization.
- Deploy as a web app (Flask/Streamlit)
